{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying Skin Lesions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCknI9hOOTj6",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "\n",
        "(R) indicates an external resource(e.g. websites, articles). These are resources that assisted me in overcoming many pain-points while building this project, which usually are articles that provide an intuitive explaination for the functions used(e.g. the map function from pandas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiQrk0qH_ivG",
        "colab_type": "code",
        "outputId": "3e8ab6f8-bfa5-4ad3-fa7c-b60a422d6c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.optimizers import *\n",
        "from keras.metrics import *\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeYbv3H1lE-J",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWAAIMOC55t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mounting google drive -> Google Colab Only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igfNY_DjENSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary to convert abbreviation of lesion types to full lesion type\n",
        "type_dictionary = {\n",
        "    'akiec' : 'Actinic Keratoses',\n",
        "    'bcc' : 'Basal cell carcinoma',\n",
        "    'bkl' : 'Benign keratosis',\n",
        "    'df' : 'Dermatofibroma',\n",
        "    'nv' : 'Melanocytic nevi',\n",
        "    'mel' : 'Melanoma',\n",
        "    'vasc' : 'Vascular skin lesions',\n",
        "}\n",
        "\n",
        "#root directory to HAM10000 dataset\n",
        "root_dir = '/content/drive/My Drive/Datasets/HAM10000/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTRbQFeCDQGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metadata dataframe\n",
        "df = pd.read_csv('/content/drive/My Drive/Datasets/HAM10000/HAM10000_metadata.csv')\n",
        "\n",
        "#make the column 'dx'(lesion type) the category datatype\n",
        "df['dx']=df['dx'].astype('category')\n",
        "\n",
        "#create a label column using the integer representation of the category / type of lesion\n",
        "df['label']=df['dx'].cat.codes\n",
        "\n",
        "#create a type column using 'dx'(abbreviation of lesion type) & map the values to full lesion type\n",
        "df['type'] = df['dx'].map(type_dictionary)\n",
        "\n",
        "df['path'] = root_dir + 'images/'+ df['image_id'].astype(str) + '.jpg'\n",
        "\n",
        "#drop useless columns -> we are only predicting the type of lesion\n",
        "df = df.drop(columns=['age', 'image_id', 'lesion_id', 'sex', 'dx_type', 'dx', 'localization'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "253zbLdZDr2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating image column\n",
        "#(R) map function w lambdas: https://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.Series.map.html\n",
        "df['image'] = df['path'].map(lambda x: np.array(Image.open(x).resize((100,75))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ5XqdldTZWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_pickle(root_dir+\"/dataframe.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tucoO0kORDSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_pickle(root_dir+\"/dataframe.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwHoItkaPErq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create training & testing tensors for features & labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['image'], df['label'], test_size = 0.2)\n",
        "\n",
        "#Create training & validation tensors\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = 0.1)\n",
        "\n",
        "#returns number of unique classes\n",
        "num_classes = y_train.unique().size\n",
        "\n",
        "#converting to np arrays\n",
        "X_train = np.array(X_train.tolist())\n",
        "X_validate = np.array(X_validate.tolist())\n",
        "X_test = np.array(X_test.tolist())\n",
        "# y_train = np.array(y_train)\n",
        "# y_validate = np.array(y_validate)\n",
        "# y_test = np.array(y_test)\n",
        "\n",
        "# Normalisation\n",
        "X_train = X_train/255\n",
        "X_validate = X_validate/255\n",
        "X_test = X_test/255\n",
        "\n",
        "#Re-shaping data\n",
        "X_train = X_train.reshape(X_train.shape[0], 75, 100, 3)\n",
        "X_validate = X_validate.reshape(X_validate.shape[0], 75, 100, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 75, 100, 3)\n",
        "\n",
        "#One-hot-encoding for labels\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_validate = np_utils.to_categorical(y_validate, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAQCZeZHQ7nR",
        "colab_type": "text"
      },
      "source": [
        "# **Problem:** Data Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veI2zgOhRJD7",
        "colab_type": "code",
        "outputId": "ce3f72be-e371-4551-cf94-818a226ee495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "#df['type'].value_counts() returns the number of time each type of skin lesion appears in the dataset\n",
        "#After plotting the data, it's clear there's a data imbalance\n",
        "#(R) Learn more about plotting here: https://datatofish.com/plot-dataframe-pandas/ \n",
        "df['type'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcf6c2801d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAFaCAYAAADy0I3fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEfwnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShS1vHjKXeu4",
        "colab_type": "text"
      },
      "source": [
        "Because the data is unbalanced across all classes, the model will have a bias towards classes like \"Melanocytic nevi\".\n",
        "\n",
        "(R) Here's an intuitive explaination for why data imbalance is bad: https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
        "\n",
        "To balance the dataset, we will have to re-sample / essentially duplicate the less frequent samples, so that all classes have the same amount of samples. This method is called oversampling.\n",
        "\n",
        "Or even better, we can use oversampling and Keras' ImageDataGenerator to augment images, this way you are introducing new data into the dataset instead just duplicating existing samples!\n",
        "\n",
        "(R) Solution of oversampling + data augmentation was proposed here: https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajiAeh5LThYh",
        "colab_type": "code",
        "outputId": "915d7439-b11f-45f0-9508-2b47ffecc3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#A custom class to generate images on-the-fly to balance a given dataset\n",
        "#Code from: https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9\n",
        "from keras.utils.data_utils import Sequence\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.keras import balanced_batch_generator\n",
        "\n",
        "class BalancedDataGenerator(Sequence):\n",
        "    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n",
        "    def __init__(self, x, y, datagen, batch_size=32):\n",
        "        self.datagen = datagen\n",
        "        self.batch_size = batch_size\n",
        "        self._shape = x.shape        \n",
        "        datagen.fit(x)\n",
        "        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._shape[0] // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_batch, y_batch = self.gen.__next__()\n",
        "        x_batch = x_batch.reshape(-1, *self._shape[1:])\n",
        "        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxWBNN6BT1BA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data augmentation + normalization\n",
        "datagen = ImageDataGenerator()\n",
        "balanced_gen = BalancedDataGenerator(X_train, y_train, datagen, batch_size=32)\n",
        "steps_per_epoch = balanced_gen.steps_per_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeLVrHxzl9-_",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_cOo5SRl-81",
        "colab_type": "code",
        "outputId": "d328c07f-74a6-43a8-c5f7-0dd0a9cc81fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "source": [
        "#(R) Model performance stats: https://github.com/jcjohnson/cnn-benchmarks\n",
        "#^Chose ResNet because it performs the best according to the above benchmarks\n",
        "resnet_model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_shape=(75, 100, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf9WaA3RsI19",
        "colab_type": "code",
        "outputId": "417c323c-5a34-4cbc-b0ca-2933fb473d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#setting the last layer of resnet to be a flatten layer\n",
        "output = resnet_model.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "resnet_model = keras.Model(resnet_model.input, output=output)\n",
        "\n",
        "#make layers untrainable\n",
        "for layer in resnet_model.layers:\n",
        "  layer.trainable=False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(Tensor(\"in..., outputs=Tensor(\"fl...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDj0GGOEnK7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creates model w 2 dense layers after resnet\n",
        "#dropout is used to prevent overfitting\n",
        "model = Sequential()\n",
        "model.add(resnet_model)\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r55urP2h-Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdMCvw2Utt0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgqMlzZ3W9Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzUIR2J71h7J",
        "colab_type": "code",
        "outputId": "e404b2fd-80b8-427b-a1bb-7ec3fae7f4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.load_weights(root_dir+'weights')\n",
        "model.fit_generator(balanced_gen, validation_data=(X_validate, y_validate), epochs=100, verbose=1)\n",
        "model.save_weights(root_dir+'weights')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "225/225 [==============================] - 196s 873ms/step - loss: 0.2308 - acc: 0.9379 - val_loss: 1.4918 - val_acc: 0.6870\n",
            "Epoch 2/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.2107 - acc: 0.9443 - val_loss: 1.1920 - val_acc: 0.6870\n",
            "Epoch 3/100\n",
            "225/225 [==============================] - 190s 842ms/step - loss: 0.1769 - acc: 0.9504 - val_loss: 1.6016 - val_acc: 0.6870\n",
            "Epoch 4/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.1448 - acc: 0.9587 - val_loss: 1.1902 - val_acc: 0.6870\n",
            "Epoch 5/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.1114 - acc: 0.9672 - val_loss: 1.2595 - val_acc: 0.6870\n",
            "Epoch 6/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.1255 - acc: 0.9612 - val_loss: 2.2068 - val_acc: 0.6870\n",
            "Epoch 7/100\n",
            "225/225 [==============================] - 191s 849ms/step - loss: 0.1057 - acc: 0.9678 - val_loss: 1.6584 - val_acc: 0.6870\n",
            "Epoch 8/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.1071 - acc: 0.9718 - val_loss: 2.4397 - val_acc: 0.6870\n",
            "Epoch 9/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0945 - acc: 0.9714 - val_loss: 2.3388 - val_acc: 0.6870\n",
            "Epoch 10/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0810 - acc: 0.9758 - val_loss: 2.9487 - val_acc: 0.6870\n",
            "Epoch 11/100\n",
            "225/225 [==============================] - 191s 847ms/step - loss: 0.1003 - acc: 0.9734 - val_loss: 2.2633 - val_acc: 0.6870\n",
            "Epoch 12/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.1109 - acc: 0.9721 - val_loss: 2.4347 - val_acc: 0.6870\n",
            "Epoch 13/100\n",
            "225/225 [==============================] - 189s 842ms/step - loss: 0.1147 - acc: 0.9704 - val_loss: 1.6143 - val_acc: 0.6870\n",
            "Epoch 14/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.1237 - acc: 0.9683 - val_loss: 1.3193 - val_acc: 0.6870\n",
            "Epoch 15/100\n",
            "225/225 [==============================] - 191s 850ms/step - loss: 0.0994 - acc: 0.9742 - val_loss: 1.5792 - val_acc: 0.6870\n",
            "Epoch 16/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.1039 - acc: 0.9736 - val_loss: 1.3674 - val_acc: 0.6870\n",
            "Epoch 17/100\n",
            "225/225 [==============================] - 191s 849ms/step - loss: 0.1123 - acc: 0.9714 - val_loss: 1.7899 - val_acc: 0.1135\n",
            "Epoch 18/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0974 - acc: 0.9756 - val_loss: 1.2662 - val_acc: 0.6870\n",
            "Epoch 19/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0854 - acc: 0.9771 - val_loss: 1.1852 - val_acc: 0.6870\n",
            "Epoch 20/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0711 - acc: 0.9807 - val_loss: 1.3729 - val_acc: 0.6870\n",
            "Epoch 21/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0809 - acc: 0.9788 - val_loss: 1.3265 - val_acc: 0.6870\n",
            "Epoch 22/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0737 - acc: 0.9796 - val_loss: 1.1979 - val_acc: 0.6870\n",
            "Epoch 23/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0731 - acc: 0.9804 - val_loss: 1.6045 - val_acc: 0.1471\n",
            "Epoch 24/100\n",
            "225/225 [==============================] - 191s 848ms/step - loss: 0.0901 - acc: 0.9796 - val_loss: 1.3189 - val_acc: 0.6870\n",
            "Epoch 25/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0763 - acc: 0.9815 - val_loss: 1.7948 - val_acc: 0.6870\n",
            "Epoch 26/100\n",
            "225/225 [==============================] - 191s 848ms/step - loss: 0.0703 - acc: 0.9801 - val_loss: 1.9032 - val_acc: 0.6870\n",
            "Epoch 27/100\n",
            "225/225 [==============================] - 191s 847ms/step - loss: 0.0728 - acc: 0.9811 - val_loss: 1.1917 - val_acc: 0.6870\n",
            "Epoch 28/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0840 - acc: 0.9782 - val_loss: 1.5381 - val_acc: 0.6870\n",
            "Epoch 29/100\n",
            "225/225 [==============================] - 190s 845ms/step - loss: 0.0647 - acc: 0.9831 - val_loss: 2.0824 - val_acc: 0.6870\n",
            "Epoch 30/100\n",
            "225/225 [==============================] - 191s 847ms/step - loss: 0.0682 - acc: 0.9812 - val_loss: 1.3727 - val_acc: 0.6870\n",
            "Epoch 31/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0522 - acc: 0.9869 - val_loss: 1.2411 - val_acc: 0.6870\n",
            "Epoch 32/100\n",
            "225/225 [==============================] - 190s 846ms/step - loss: 0.0542 - acc: 0.9871 - val_loss: 1.7304 - val_acc: 0.6870\n",
            "Epoch 33/100\n",
            "225/225 [==============================] - 190s 847ms/step - loss: 0.0674 - acc: 0.9849 - val_loss: 2.0369 - val_acc: 0.6870\n",
            "Epoch 34/100\n",
            "225/225 [==============================] - 192s 852ms/step - loss: 0.0546 - acc: 0.9853 - val_loss: 1.9389 - val_acc: 0.6870\n",
            "Epoch 35/100\n",
            "225/225 [==============================] - 192s 853ms/step - loss: 0.0814 - acc: 0.9811 - val_loss: 1.9161 - val_acc: 0.6870\n",
            "Epoch 36/100\n",
            "225/225 [==============================] - 191s 850ms/step - loss: 0.0712 - acc: 0.9810 - val_loss: 1.4831 - val_acc: 0.6870\n",
            "Epoch 37/100\n",
            "225/225 [==============================] - 191s 848ms/step - loss: 0.0650 - acc: 0.9833 - val_loss: 1.6694 - val_acc: 0.6870\n",
            "Epoch 38/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0561 - acc: 0.9861 - val_loss: 1.8225 - val_acc: 0.6870\n",
            "Epoch 39/100\n",
            "225/225 [==============================] - 192s 854ms/step - loss: 0.0394 - acc: 0.9903 - val_loss: 1.5610 - val_acc: 0.6870\n",
            "Epoch 40/100\n",
            "225/225 [==============================] - 191s 847ms/step - loss: 0.0577 - acc: 0.9864 - val_loss: 1.9792 - val_acc: 0.6870\n",
            "Epoch 41/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0616 - acc: 0.9853 - val_loss: 1.9748 - val_acc: 0.6870\n",
            "Epoch 42/100\n",
            "225/225 [==============================] - 191s 850ms/step - loss: 0.0527 - acc: 0.9881 - val_loss: 2.5587 - val_acc: 0.0349\n",
            "Epoch 43/100\n",
            "225/225 [==============================] - 191s 849ms/step - loss: 0.0564 - acc: 0.9873 - val_loss: 1.5781 - val_acc: 0.6870\n",
            "Epoch 44/100\n",
            "225/225 [==============================] - 190s 847ms/step - loss: 0.0692 - acc: 0.9839 - val_loss: 1.6261 - val_acc: 0.6845\n",
            "Epoch 45/100\n",
            "225/225 [==============================] - 191s 849ms/step - loss: 0.0866 - acc: 0.9819 - val_loss: 1.9158 - val_acc: 0.6870\n",
            "Epoch 46/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0643 - acc: 0.9828 - val_loss: 1.1780 - val_acc: 0.6870\n",
            "Epoch 47/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0621 - acc: 0.9854 - val_loss: 1.1198 - val_acc: 0.6870\n",
            "Epoch 48/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0511 - acc: 0.9885 - val_loss: 1.6324 - val_acc: 0.6870\n",
            "Epoch 49/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0561 - acc: 0.9860 - val_loss: 1.4321 - val_acc: 0.6870\n",
            "Epoch 50/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0579 - acc: 0.9875 - val_loss: 1.3015 - val_acc: 0.6870\n",
            "Epoch 51/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0584 - acc: 0.9858 - val_loss: 1.4187 - val_acc: 0.6870\n",
            "Epoch 52/100\n",
            "225/225 [==============================] - 189s 842ms/step - loss: 0.0503 - acc: 0.9887 - val_loss: 1.8461 - val_acc: 0.6870\n",
            "Epoch 53/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0652 - acc: 0.9853 - val_loss: 1.5628 - val_acc: 0.6870\n",
            "Epoch 54/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0593 - acc: 0.9865 - val_loss: 1.5615 - val_acc: 0.6870\n",
            "Epoch 55/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0392 - acc: 0.9907 - val_loss: 1.5997 - val_acc: 0.6870\n",
            "Epoch 56/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0499 - acc: 0.9896 - val_loss: 1.4326 - val_acc: 0.6870\n",
            "Epoch 57/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0516 - acc: 0.9878 - val_loss: 3.8796 - val_acc: 0.0087\n",
            "Epoch 58/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0488 - acc: 0.9893 - val_loss: 1.5312 - val_acc: 0.6870\n",
            "Epoch 59/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0286 - acc: 0.9924 - val_loss: 1.5011 - val_acc: 0.6870\n",
            "Epoch 60/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0454 - acc: 0.9901 - val_loss: 1.4245 - val_acc: 0.6197\n",
            "Epoch 61/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0487 - acc: 0.9904 - val_loss: 1.5864 - val_acc: 0.6870\n",
            "Epoch 62/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0705 - acc: 0.9843 - val_loss: 1.9176 - val_acc: 0.6870\n",
            "Epoch 63/100\n",
            "225/225 [==============================] - 189s 842ms/step - loss: 0.0516 - acc: 0.9897 - val_loss: 1.6672 - val_acc: 0.6870\n",
            "Epoch 64/100\n",
            "225/225 [==============================] - 188s 838ms/step - loss: 0.0514 - acc: 0.9879 - val_loss: 1.5568 - val_acc: 0.6870\n",
            "Epoch 65/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0493 - acc: 0.9897 - val_loss: 1.4574 - val_acc: 0.6870\n",
            "Epoch 66/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0566 - acc: 0.9875 - val_loss: 1.4076 - val_acc: 0.6870\n",
            "Epoch 67/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0522 - acc: 0.9894 - val_loss: 1.4636 - val_acc: 0.6671\n",
            "Epoch 68/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0574 - acc: 0.9879 - val_loss: 1.5701 - val_acc: 0.6870\n",
            "Epoch 69/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0364 - acc: 0.9932 - val_loss: 1.8720 - val_acc: 0.6870\n",
            "Epoch 70/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0430 - acc: 0.9903 - val_loss: 1.5503 - val_acc: 0.6870\n",
            "Epoch 71/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0463 - acc: 0.9900 - val_loss: 2.2257 - val_acc: 0.0636\n",
            "Epoch 72/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0575 - acc: 0.9885 - val_loss: 1.5281 - val_acc: 0.6858\n",
            "Epoch 73/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0642 - acc: 0.9889 - val_loss: 1.2998 - val_acc: 0.6870\n",
            "Epoch 74/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0390 - acc: 0.9913 - val_loss: 1.4533 - val_acc: 0.6870\n",
            "Epoch 75/100\n",
            "225/225 [==============================] - 190s 842ms/step - loss: 0.0389 - acc: 0.9914 - val_loss: 1.3688 - val_acc: 0.6870\n",
            "Epoch 76/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0560 - acc: 0.9874 - val_loss: 1.4407 - val_acc: 0.6870\n",
            "Epoch 77/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0369 - acc: 0.9913 - val_loss: 1.6296 - val_acc: 0.6870\n",
            "Epoch 78/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0379 - acc: 0.9917 - val_loss: 1.2642 - val_acc: 0.6870\n",
            "Epoch 79/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0388 - acc: 0.9914 - val_loss: 1.3822 - val_acc: 0.6721\n",
            "Epoch 80/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0381 - acc: 0.9922 - val_loss: 1.4059 - val_acc: 0.6870\n",
            "Epoch 81/100\n",
            "225/225 [==============================] - 189s 840ms/step - loss: 0.0271 - acc: 0.9936 - val_loss: 1.4132 - val_acc: 0.6870\n",
            "Epoch 82/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0338 - acc: 0.9939 - val_loss: 1.7309 - val_acc: 0.6870\n",
            "Epoch 83/100\n",
            "225/225 [==============================] - 190s 846ms/step - loss: 0.0361 - acc: 0.9932 - val_loss: 1.6870 - val_acc: 0.6870\n",
            "Epoch 84/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0566 - acc: 0.9903 - val_loss: 3.3700 - val_acc: 0.1047\n",
            "Epoch 85/100\n",
            "225/225 [==============================] - 188s 835ms/step - loss: 0.0352 - acc: 0.9929 - val_loss: 2.6777 - val_acc: 0.6870\n",
            "Epoch 86/100\n",
            "225/225 [==============================] - 190s 845ms/step - loss: 0.0491 - acc: 0.9911 - val_loss: 2.6442 - val_acc: 0.6870\n",
            "Epoch 87/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0518 - acc: 0.9913 - val_loss: 1.9147 - val_acc: 0.6870\n",
            "Epoch 88/100\n",
            "225/225 [==============================] - 190s 844ms/step - loss: 0.0334 - acc: 0.9931 - val_loss: 2.0758 - val_acc: 0.6870\n",
            "Epoch 89/100\n",
            "225/225 [==============================] - 188s 837ms/step - loss: 0.0375 - acc: 0.9925 - val_loss: 2.3763 - val_acc: 0.6870\n",
            "Epoch 90/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0472 - acc: 0.9911 - val_loss: 2.7966 - val_acc: 0.6870\n",
            "Epoch 91/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0431 - acc: 0.9914 - val_loss: 2.4879 - val_acc: 0.6870\n",
            "Epoch 92/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0318 - acc: 0.9933 - val_loss: 1.7836 - val_acc: 0.6870\n",
            "Epoch 93/100\n",
            "225/225 [==============================] - 188s 835ms/step - loss: 0.0405 - acc: 0.9914 - val_loss: 2.6480 - val_acc: 0.6870\n",
            "Epoch 94/100\n",
            "225/225 [==============================] - 187s 833ms/step - loss: 0.0433 - acc: 0.9924 - val_loss: 2.4079 - val_acc: 0.6870\n",
            "Epoch 95/100\n",
            "225/225 [==============================] - 189s 839ms/step - loss: 0.0472 - acc: 0.9910 - val_loss: 2.1213 - val_acc: 0.6870\n",
            "Epoch 96/100\n",
            "225/225 [==============================] - 189s 841ms/step - loss: 0.0637 - acc: 0.9901 - val_loss: 2.3453 - val_acc: 0.6870\n",
            "Epoch 97/100\n",
            "225/225 [==============================] - 190s 843ms/step - loss: 0.0389 - acc: 0.9928 - val_loss: 1.8090 - val_acc: 0.6870\n",
            "Epoch 98/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0432 - acc: 0.9913 - val_loss: 3.3718 - val_acc: 0.6870\n",
            "Epoch 99/100\n",
            "225/225 [==============================] - 188s 836ms/step - loss: 0.0405 - acc: 0.9919 - val_loss: 2.7303 - val_acc: 0.6870\n",
            "Epoch 100/100\n",
            "225/225 [==============================] - 189s 838ms/step - loss: 0.0453 - acc: 0.9910 - val_loss: 2.4612 - val_acc: 0.6870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX-mkktovfCV",
        "colab_type": "code",
        "outputId": "7ecd7591-52d8-4ea1-b0fd-d9f97c87c0a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2003/2003 [==============================] - 49s 25ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6618104976115082, 0.680978531963636]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}